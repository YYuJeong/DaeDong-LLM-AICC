{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.40.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (5.4.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (11.0.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (4.25.5)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (18.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.5.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting chatbot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile chatbot.py\n",
    "\n",
    "import os\n",
    "import streamlit as st\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader, DataFrameLoader\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import tempfile\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "import pandas as pd\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API í‚¤ ì •ë³´ ë¡œë“œ\n",
    "load_dotenv()\n",
    "########## 1. í´ë” ë‚´ íŒŒì¼ ë¡œë“œ ##########\n",
    "\n",
    "# í´ë” ê²½ë¡œ ì„¤ì •\n",
    "folder_path = \"./data\"  # ë¶„ì„í•  íŒŒì¼ì´ ì €ì¥ëœ í´ë” ê²½ë¡œ\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# PDF ë¬¸ì„œ ë¡œë“œ í•¨ìˆ˜\n",
    "def load_pdf_with_metadata(file_path):\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    documents = loader.load_and_split(text_splitter)\n",
    "    for doc in documents:\n",
    "        doc.metadata[\"source\"] = os.path.basename(file_path)\n",
    "        doc.metadata[\"page\"] = doc.metadata.get(\"page\", \"Unknown\")\n",
    "    return documents\n",
    "\n",
    "# ì—‘ì…€ ë¬¸ì„œ ë¡œë“œ í•¨ìˆ˜\n",
    "def load_excel_with_metadata(file_path):\n",
    "    documents = []\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "        loader = DataFrameLoader(df, page_content_column=df.columns[0])\n",
    "        sheet_docs = loader.load_and_split(text_splitter)\n",
    "        for doc in sheet_docs:\n",
    "            doc.metadata[\"source\"] = os.path.basename(file_path)\n",
    "            doc.metadata[\"sheet_name\"] = sheet_name\n",
    "            doc.metadata[\"cell_range\"] = f\"A1:{df.columns[-1]}{len(df)}\"  # ì¶”ê°€ ì…€ ë²”ìœ„ ì •ë³´\n",
    "        documents.extend(sheet_docs)\n",
    "    return documents\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "def load_csv_with_metadata(file_path):\n",
    "    documents = []\n",
    "    \n",
    "    # íŒŒì¼ ì¸ì½”ë”© ìë™ ê°ì§€\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        detected_encoding = chardet.detect(f.read())[\"encoding\"]\n",
    "    \n",
    "    # ê°ì§€ëœ ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ ì½ê¸°\n",
    "    df = pd.read_csv(file_path, encoding=detected_encoding)\n",
    "    \n",
    "    # NaN ê°’ ì²˜ë¦¬ (ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´)\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    \n",
    "    loader = DataFrameLoader(df, page_content_column=df.columns[0])\n",
    "    csv_docs = loader.load_and_split(text_splitter)\n",
    "    \n",
    "    for doc in csv_docs:\n",
    "        doc.metadata[\"source\"] = os.path.basename(file_path)\n",
    "        doc.metadata[\"cell_range\"] = f\"A1:{df.columns[-1]}{len(df)}\"  # ì¶”ê°€ ì…€ ë²”ìœ„ ì •ë³´\n",
    "    documents.extend(csv_docs)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "# # CSV ë¬¸ì„œ ë¡œë“œ í•¨ìˆ˜\n",
    "# def load_csv_with_metadata(file_path):\n",
    "#     documents = []\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     loader = DataFrameLoader(df, page_content_column=df.columns[0])\n",
    "#     csv_docs = loader.load_and_split(text_splitter)\n",
    "#     for doc in csv_docs:\n",
    "#         doc.metadata[\"source\"] = os.path.basename(file_path)\n",
    "#         doc.metadata[\"cell_range\"] = f\"A1:{df.columns[-1]}{len(df)}\"  # ì¶”ê°€ ì…€ ë²”ìœ„ ì •ë³´\n",
    "#     documents.extend(csv_docs)\n",
    "#     return documents\n",
    "\n",
    "# í´ë” ë‚´ ëª¨ë“  ë¬¸ì„œë¥¼ ë¡œë“œ\n",
    "\n",
    "def load_documents_from_folder(folder_path):\n",
    "    documents = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            documents.extend(load_pdf_with_metadata(file_path))\n",
    "        elif file_name.endswith(\".xlsx\") or file_name.endswith(\".xls\"):\n",
    "            documents.extend(load_excel_with_metadata(file_path))\n",
    "        elif file_name.endswith(\".csv\"):\n",
    "            documents.extend(load_csv_with_metadata(file_path))\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "# ì—ì´ì „íŠ¸ì™€ ëŒ€í™”í•˜ëŠ” í•¨ìˆ˜\n",
    "def chat_with_agent(user_input, agent_executor):\n",
    "    result = agent_executor({\"input\": user_input})\n",
    "    response = result['output']  # ëª…ì‹œì ìœ¼ë¡œ ì¶œë ¥ í‚¤ë¥¼ ì²˜ë¦¬\n",
    "    return response\n",
    "\n",
    "# ì„¸ì…˜ ê¸°ë¡ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "def get_session_history(session_ids):\n",
    "    if session_ids not in st.session_state.session_history:\n",
    "        st.session_state.session_history[session_ids] = ChatMessageHistory()\n",
    "    return st.session_state.session_history[session_ids]\n",
    "\n",
    "# ëŒ€í™” ë‚´ìš© ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜\n",
    "def print_messages():\n",
    "    for msg in st.session_state[\"messages\"]:\n",
    "        st.chat_message(msg['role']).write(msg['content'])\n",
    "\n",
    "\n",
    "# ëª¨ë“  ë¬¸ì„œ ë¡œë“œ\n",
    "all_docs = load_documents_from_folder(folder_path)\n",
    "\n",
    "\n",
    "# FAISS ì¸ë±ìŠ¤ ì„¤ì • ë° ìƒì„±\n",
    "vector = FAISS.from_documents(all_docs, OpenAIEmbeddings())\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "# ë„êµ¬ ì •ì˜\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"pdf_search\",\n",
    "    description=\"Use this tool to search information from the pdf document\"\n",
    ")\n",
    "\n",
    "# Streamlit ë©”ì¸ ì½”ë“œ\n",
    "def main():\n",
    "    # í˜ì´ì§€ ì„¤ì •\n",
    "    st.set_page_config(page_title=\"AI ë¹„ì„œ\", layout=\"wide\", page_icon=\"ğŸ¤–\")\n",
    "\n",
    "    st.image('Cute_Robot_Tractor_with_Label.png', width=500)\n",
    "    st.markdown('---')\n",
    "    st.title(\"ì•ˆë…•í•˜ì„¸ìš”! RAGë¥¼ í™œìš©í•œ 'AICC' ì…ë‹ˆë‹¤\")  # ì‹œì‘ íƒ€ì´í‹€\n",
    "\n",
    "    # ì„¸ì…˜ ì´ˆê¸°í™”\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state[\"messages\"] = []\n",
    "\n",
    "    if \"session_history\" not in st.session_state:\n",
    "        st.session_state[\"session_history\"] = {}\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.session_state[\"OPENAI_API\"] = st.text_input(label=\"OPENAI API í‚¤\", placeholder=\"Enter Your API Key\", value=\"\", type=\"password\")\n",
    "        st.markdown('---')\n",
    "\n",
    "    # OpenAI API í‚¤ê°€ ì…ë ¥ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "    if st.session_state[\"OPENAI_API\"] :\n",
    "        os.environ['OPENAI_API_KEY'] = st.session_state[\"OPENAI_API\"]\n",
    "\n",
    "# return retriever_tool\n",
    "        tools = [retriever_tool]\n",
    "\n",
    "        # LLM ì„¤ì •\n",
    "        llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "        # Prompt ì •ì˜\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are a friendly and professional expert with over 10 years of experience at AICC. Your role is to respond to internal staff inquiries in a professional yet approachable manner. All responses should be written in Korean.\n",
    "\n",
    "                    You specialize in resolving customer issues, providing product and service information, offering technical support, and guiding staff through various processes. Your current role involves assisting internal staff in handling customer inquiries effectively, leveraging given data to promptly provide solutions.\n",
    "\n",
    "                    Analyze the given DataFrame (df) containing the columns Inquiry Type, Customer Name, Inquiry Content, and Received Date. Categorize each inquiry into one of the following categories: [Product Information], [Service Issue], [Technical Support], or [Other]. Additionally, summarize the main points of the customer inquiry and suggest an appropriate solution for internal staff.\n",
    "                    Please always include emojis in your responses with a friendly tone.\n",
    "                    When the chat begins, first introduce yourself and your goal, and then request information about the type of data you will be processing or any additional details you might need. If the keywords provided by the user do not match the predefined categories, clarify the user's intent, verify if the keywords are suitable, and request clarification if necessary.\n",
    "\n",
    "                    Responses should follow the format below:\n",
    "                        \n",
    "                    \"Your name is `AICC ë„ì›€ì´`. Please introduce yourself at the beginning of the conversation.\" \n",
    "\n",
    "                    #FORMAT\n",
    "\n",
    "                     * ë¬¸ì˜ ìœ í˜•\n",
    "                      -\n",
    "\n",
    "                       * ë¬¸ì˜ ë‹µë³€\n",
    "                      -\n",
    "                      -\n",
    "                      -              \n",
    "                        \n",
    "                    \"\"\"\n",
    "\n",
    "                ),\n",
    "                (\"placeholder\", \"{chat_history}\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "                (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # ì—ì´ì „íŠ¸ ìƒì„± (initialize_agent ëŒ€ì‹  create_tool_calling_agent ì‚¬ìš©)\n",
    "        agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "        # AgentExecutor ì •ì˜\n",
    "        agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "        # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬\n",
    "        user_input = st.chat_input('ì§ˆë¬¸ì´ ë¬´ì—‡ì¸ê°€ìš”?')\n",
    "\n",
    "        if user_input:\n",
    "            session_id = \"default_session\"\n",
    "            session_history = get_session_history(session_id)\n",
    "\n",
    "            if session_history.messages:\n",
    "                previous_messages = [{\"role\": msg['role'], \"content\": msg['content']} for msg in session_history.messages]\n",
    "                response = chat_with_agent(user_input + \"\\n\\nPrevious Messages: \" + str(previous_messages), agent_executor)\n",
    "            else:\n",
    "                response = chat_with_agent(user_input, agent_executor)\n",
    "\n",
    "            # ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ì— ì¶”ê°€\n",
    "            st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": user_input})\n",
    "            st.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "            # ì„¸ì…˜ ê¸°ë¡ì— ë©”ì‹œì§€ë¥¼ ì¶”ê°€\n",
    "            session_history.add_message({\"role\": \"user\", \"content\": user_input})\n",
    "            session_history.add_message({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "        # ëŒ€í™” ë‚´ìš© ì¶œë ¥\n",
    "        print_messages()\n",
    "\n",
    "    else:\n",
    "        st.warning(\"OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in c:\\users\\thrje\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ympy (c:\\Users\\thrje\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement PyMuPDFLoader (from versions: none)\n",
      "ERROR: No matching distribution found for PyMuPDFLoader\n"
     ]
    }
   ],
   "source": [
    "pip install chardet PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting chatbot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile chatbot.py\n",
    "\n",
    "from tabulate import tabulate\n",
    "import chardet\n",
    "import os\n",
    "import streamlit as st\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader, DataFrameLoader\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import tempfile\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "import pandas as pd\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API í‚¤ ì •ë³´ ë¡œë“œ\n",
    "load_dotenv()\n",
    "########## 1. í´ë” ë‚´ íŒŒì¼ ë¡œë“œ ##########\n",
    "\n",
    "# í´ë” ê²½ë¡œ ì„¤ì •\n",
    "folder_path = \"./data\"  # ë¶„ì„í•  íŒŒì¼ì´ ì €ì¥ëœ í´ë” ê²½ë¡œ\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)\n",
    "\n",
    "# PDF ë¬¸ì„œ ë¡œë“œ í•¨ìˆ˜\n",
    "def load_pdf_with_metadata(file_path):\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    documents = loader.load_and_split(text_splitter)\n",
    "    for doc in documents:\n",
    "        doc.metadata[\"source\"] = os.path.basename(file_path)\n",
    "        doc.metadata[\"page\"] = doc.metadata.get(\"page\", \"Unknown\")\n",
    "    return documents\n",
    "\n",
    "# ì—‘ì…€ ë¬¸ì„œ ë¡œë“œ í•¨ìˆ˜\n",
    "def load_excel_with_metadata(file_path):\n",
    "    documents = []\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "        loader = DataFrameLoader(df, page_content_column=df.columns[0])\n",
    "        sheet_docs = loader.load_and_split(text_splitter)\n",
    "        for doc in sheet_docs:\n",
    "            doc.metadata[\"source\"] = os.path.basename(file_path)\n",
    "            doc.metadata[\"sheet_name\"] = sheet_name\n",
    "            doc.metadata[\"cell_range\"] = f\"A1:{df.columns[-1]}{len(df)}\"  # ì¶”ê°€ ì…€ ë²”ìœ„ ì •ë³´\n",
    "        documents.extend(sheet_docs)\n",
    "    return documents\n",
    "\n",
    "\n",
    "def load_csv_with_metadata(file_path):\n",
    "    documents = []\n",
    "    \n",
    "    # íŒŒì¼ ì¸ì½”ë”© ìë™ ê°ì§€\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        detected_encoding = chardet.detect(f.read())[\"encoding\"]\n",
    "    \n",
    "    # ê°ì§€ëœ ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ ì½ê¸°\n",
    "    df = pd.read_csv(file_path, encoding=detected_encoding)\n",
    "    \n",
    "    # NaN ê°’ ì²˜ë¦¬ (ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´)\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    \n",
    "    loader = DataFrameLoader(df, page_content_column=df.columns[0])\n",
    "    csv_docs = loader.load_and_split(text_splitter)\n",
    "    \n",
    "    for doc in csv_docs:\n",
    "        doc.metadata[\"source\"] = os.path.basename(file_path)\n",
    "        doc.metadata[\"cell_range\"] = f\"A1:{df.columns[-1]}{len(df)}\"  # ì¶”ê°€ ì…€ ë²”ìœ„ ì •ë³´\n",
    "    documents.extend(csv_docs)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# í´ë” ë‚´ ëª¨ë“  ë¬¸ì„œë¥¼ ë¡œë“œ\n",
    "\n",
    "def load_documents_from_folder(folder_path):\n",
    "    documents = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            documents.extend(load_pdf_with_metadata(file_path))\n",
    "        elif file_name.endswith(\".xlsx\") or file_name.endswith(\".xls\"):\n",
    "            documents.extend(load_excel_with_metadata(file_path))\n",
    "        elif file_name.endswith(\".csv\"):\n",
    "            documents.extend(load_csv_with_metadata(file_path))\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "# ì—ì´ì „íŠ¸ì™€ ëŒ€í™”í•˜ëŠ” í•¨ìˆ˜\n",
    "def chat_with_agent(user_input, agent_executor):\n",
    "    result = agent_executor({\"input\": user_input})\n",
    "    response = result['output']  # ëª…ì‹œì ìœ¼ë¡œ ì¶œë ¥ í‚¤ë¥¼ ì²˜ë¦¬\n",
    "    return response\n",
    "\n",
    "# ì„¸ì…˜ ê¸°ë¡ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "def get_session_history(session_ids):\n",
    "    if session_ids not in st.session_state.session_history:\n",
    "        st.session_state.session_history[session_ids] = ChatMessageHistory()\n",
    "    return st.session_state.session_history[session_ids]\n",
    "\n",
    "# ëŒ€í™” ë‚´ìš© ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜\n",
    "def print_messages():\n",
    "    for msg in st.session_state[\"messages\"]:\n",
    "        st.chat_message(msg['role']).write(msg['content'])\n",
    "\n",
    "\n",
    "# ëª¨ë“  ë¬¸ì„œ ë¡œë“œ\n",
    "all_docs = load_documents_from_folder(folder_path)\n",
    "\n",
    "\n",
    "# FAISS ì¸ë±ìŠ¤ ì„¤ì • ë° ìƒì„±\n",
    "vector = FAISS.from_documents(all_docs, OpenAIEmbeddings())\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "# ë„êµ¬ ì •ì˜\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"csv_search\",\n",
    "    description=\"Use this tool to search information from the csv document\"\n",
    ")\n",
    "\n",
    "\n",
    "QUERY_COMPARISON = {\n",
    "    \"question\": \"ëŒ€ë™ HX1400L-2Cì™€ TYM T70ì˜ ì°¨ì´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "    \"type\": \"ì¼ë°˜ ì •ë³´ ë¬¸ì˜\",\n",
    "    \"response\": {\n",
    "        \"engine_output_and_performance\": {\n",
    "            \"HX1400L-2C\": {\n",
    "                \"power\": \"142ë§ˆë ¥(PS)\",\n",
    "                \"engine\": \"3,833cc ë°°ê¸°ëŸ‰ì˜ 4ê¸°í†µ ë””ì ¤ ì—”ì§„\",\n",
    "                \"description\": \"ê°•ë ¥í•œ ì—”ì§„ ì¶œë ¥ê³¼ ëŒ€ê·œëª¨ ì‘ì—…ì— ì í•©\"\n",
    "            },\n",
    "            \"TYM T70\": {\n",
    "                \"power\": \"70ë§ˆë ¥(PS)\",\n",
    "                \"engine\": \"2,400cc ë°°ê¸°ëŸ‰ì˜ í„°ë³´ ì—”ì§„\",\n",
    "                \"description\": \"ì¶œë ¥ì´ ë‚®ì•„ ëŒ€ê·œëª¨ ì‘ì—…ì—ëŠ” ë¶€ì¡±\"\n",
    "            },\n",
    "            \"comparison\": \"HX1400L-2CëŠ” T70ë³´ë‹¤ ë” ê°•ë ¥í•œ ì—”ì§„ ì¶œë ¥ê³¼ ë°°ê¸°ëŸ‰ìœ¼ë¡œ ëŒ€ê·œëª¨ ì‘ì—…ì— ì í•©\"\n",
    "        },\n",
    "        \"key_features_and_characteristics\": {\n",
    "            \"HX1400L-2C\": [\n",
    "                \"ìŠ¤ë§ˆíŠ¸í° 'ëŒ€ë™ ì»¤ë„¥íŠ¸' ì„œë¹„ìŠ¤ë¡œ ì›ê²© ì œì–´ ë° ê´€ë¦¬ ê°€ëŠ¥\",\n",
    "                \"ììœ¨ ì§ì§„ ê¸°ëŠ¥ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ì§ì„  ì‘ì—… ì§€ì›\",\n",
    "                \"10ì¸ì¹˜ í„°ì¹˜ìŠ¤í¬ë¦° ëª¨ë‹ˆí„°ë¡œ ì‘ì—… ìƒíƒœ ì§ê´€ì  í™•ì¸\"\n",
    "            ],\n",
    "            \"TYM T70\": [\n",
    "                \"ê¸°ë³¸ ìœ ì•• ë°¸ë¸Œì™€ ì‘ì—… í¸ì˜ì„± ì œê³µ\",\n",
    "                \"ë””ì§€í„¸ ê¸°ëŠ¥ ë° ììœ¨ ì£¼í–‰ ê¸°ìˆ  ë¶€ì¡±\"\n",
    "            ],\n",
    "            \"comparison\": \"HX1400L-2CëŠ” ì²¨ë‹¨ ê¸°ìˆ ê³¼ ë””ì§€í„¸ ê´€ë¦¬ ì‹œìŠ¤í…œìœ¼ë¡œ ì‚¬ìš© í¸ì˜ì„±ê³¼ ì‘ì—… íš¨ìœ¨ì„±ì´ ë›°ì–´ë‚¨\"\n",
    "        },\n",
    "        \"usage_and_suitability\": {\n",
    "            \"HX1400L-2C\": {\n",
    "                \"description\": \"ëŒ€ê·œëª¨ ë†ì—… ì‘ì—…ì— ì´ìƒì \",\n",
    "                \"applications\": [\"ëŒ€ê·œëª¨ ê²½ì‘ì§€\", \"ì¶•ì‚° ë†ê°€\"],\n",
    "                \"benefits\": \"ê³ ì¶œë ¥ê³¼ ììœ¨ ì§ì§„ìœ¼ë¡œ ì‘ì—… ì‹œê°„ ë‹¨ì¶•\"\n",
    "            },\n",
    "            \"TYM T70\": {\n",
    "                \"description\": \"ì¤‘ì†Œê·œëª¨ ë†ì—… ì‘ì—…ì— ì í•©\",\n",
    "                \"limitations\": \"ê³ ì¶œë ¥ ì‘ì—…ì—ëŠ” ì í•©í•˜ì§€ ì•ŠìŒ\"\n",
    "            },\n",
    "            \"comparison\": \"HX1400L-2CëŠ” ëŒ€ê·œëª¨ ì‘ì—…ì— ìµœì í™”ëœ ì„±ëŠ¥ê³¼ ê¸°ìˆ  ì œê³µ\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "QUERY_INFO = {\n",
    "    \"question\": \"ëŒ€ë™ HX1400L-2Cì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œê³  ì‹¶ì–´ìš”.\",\n",
    "    \"type\": \"ì¼ë°˜ ì •ë³´ ë¬¸ì˜\",\n",
    "    \"answer\": {\n",
    "        \"description\": \"ëŒ€ë™ HX1400L-2CëŠ” ë‹¤ì–‘í•œ ë†ì—… ì‘ì—…ì— ìµœì í™”ëœ ê³ ì¶œë ¥ íŠ¸ë™í„°ì…ë‹ˆë‹¤.\",\n",
    "        \"details\": [\n",
    "            {\"í•­ëª©\": \"ëª¨ë¸ëª…\", \"ì„¸ë¶€ ë‚´ìš©\": \"HX1400L-2C\"},\n",
    "            {\"í•­ëª©\": \"ì—”ì§„ ì¶œë ¥\", \"ì„¸ë¶€ ë‚´ìš©\": \"142ë§ˆë ¥ (PS)\"},\n",
    "            {\"í•­ëª©\": \"ì—”ì§„ í˜•ì‹\", \"ì„¸ë¶€ ë‚´ìš©\": \"4ê¸°í†µ ë””ì ¤ ì—”ì§„\"},\n",
    "            {\"í•­ëª©\": \"ë°°ê¸°ëŸ‰\", \"ì„¸ë¶€ ë‚´ìš©\": \"3,833cc\"},\n",
    "            {\"í•­ëª©\": \"ì •ê²© íšŒì „ ì†ë„\", \"ì„¸ë¶€ ë‚´ìš©\": \"2,200rpm\"},\n",
    "            {\"í•­ëª©\": \"ìŠ¤ë§ˆíŠ¸ ê´€ë¦¬ ê¸°ëŠ¥\", \"ì„¸ë¶€ ë‚´ìš©\": \"'ëŒ€ë™ ì»¤ë„¥íŠ¸'ë¡œ ì›ê²© ì œì–´ ë° ê´€ë¦¬ ê°€ëŠ¥\"},\n",
    "            {\"í•­ëª©\": \"ì£¼í–‰ ê¸°ëŠ¥\", \"ì„¸ë¶€ ë‚´ìš©\": \"ììœ¨ ì§ì§„ ê¸°ëŠ¥ìœ¼ë¡œ í•¸ë“¤ ì¡°ì‘ ì—†ì´ ì§ì„  ì‘ì—… ê°€ëŠ¥\"},\n",
    "            {\"í•­ëª©\": \"ë””ìŠ¤í”Œë ˆì´\", \"ì„¸ë¶€ ë‚´ìš©\": \"10ì¸ì¹˜ í„°ì¹˜ìŠ¤í¬ë¦° ëª¨ë‹ˆí„°ë¡œ ì‘ì—… ìƒíƒœ í™•ì¸ ê°€ëŠ¥\"},\n",
    "            {\"í•­ëª©\": \"ìºë¹ˆ ë””ìì¸\", \"ì„¸ë¶€ ë‚´ìš©\": \"ë„“ê³  í¸ì•ˆí•œ 5ì£¼ì‹ ìºë¹ˆ, ì¸ì²´ê³µí•™ì  ì„¤ê³„\"},\n",
    "            {\"í•­ëª©\": \"ì™¸ê´€ ë””ìì¸\", \"ì„¸ë¶€ ë‚´ìš©\": \"ëŸ­ì…”ë¦¬í•œ ì™¸ê´€ê³¼ í–¥ìƒëœ í—¤ë“œë¨í”„ ë””ìì¸\"},\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "COMPARE_INFO = {\n",
    "    \"question\": \"ëŒ€ë™ì˜ LK400L5 íŠ¸ë™í„°ëŠ” LS ì— íŠ¸ë¡ ì´ë‚˜ TYM íŠ¸ë™í„°ì™€ ë¹„êµí–ˆì„ ë•Œ ì–´ë–¤ ì¥ì ì´ ìˆë‚˜ìš”?\",\n",
    "    \"type\": \"ë¹„êµ ë¶„ì„ ë¬¸ì˜\",\n",
    "    \"answer\": {\n",
    "        \"description\": \"ëŒ€ë™ì˜ LK400L5 íŠ¸ë™í„°ëŠ” íš¨ìœ¨ì ì¸ ì—”ì§„ ì„±ëŠ¥, ì‘ì—… í¸ì˜ì„±, ê¸°ë™ì„± ì¸¡ë©´ì—ì„œ LS ì— íŠ¸ë¡ ì´ë‚˜ TYM íŠ¸ë™í„°ì™€ ë¹„êµí•´ ê°•ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "        \"details\": [\n",
    "            {\"í•­ëª©\": \"ì—”ì§„ ì„±ëŠ¥\", \"ì„¸ë¶€ ë‚´ìš©\": \"29.5kW ì¶œë ¥ê³¼ 1,826cc ë°°ê¸°ëŸ‰ìœ¼ë¡œ ë™ê¸‰ ëŒ€ë¹„ ì—°ë£Œ íš¨ìœ¨ì´ ìš°ìˆ˜í•˜ë©°, ì†Œê·œëª¨ ì‘ì—… í™˜ê²½ì— ì í•©.\"},\n",
    "            {\"í•­ëª©\": \"ì‘ì—… í¸ì˜ì„±\", \"ì„¸ë¶€ ë‚´ìš©\": \"ë…ë¦½í˜• PTO ì‹œìŠ¤í…œìœ¼ë¡œ ë¶€ì† ì¥ë¹„ì™€ ë©”ì¸ íŠ¸ë™í„°ë¥¼ ë³„ë„ë¡œ ì œì–´ ê°€ëŠ¥.\"},\n",
    "            {\"í•­ëª©\": \"ê¸°ë™ì„±\", \"ì„¸ë¶€ ë‚´ìš©\": \"í­ 1,292mm, ê¸¸ì´ 3,070mmë¡œ ì¢ì€ ê³µê°„ì—ì„œë„ ìš°ìˆ˜í•œ ê¸°ë™ì„± ì œê³µ.\"},\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "TRACTOR_INFO = {\n",
    "    \"question\": \"ëŒ€ë™ íŠ¸ë™í„° ì œí’ˆêµ°ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "    \"type\": \"ì¼ë°˜ ì •ë³´ ë¬¸ì˜\",\n",
    "    \"answer\": {\n",
    "        \"description\": \"ëŒ€ë™ì€ ë‹¤ì–‘í•œ ë†ì—… í™˜ê²½ê³¼ ì‘ì—… ìš”êµ¬ì— ë¶€ì‘í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ íŠ¸ë™í„° ì‹œë¦¬ì¦ˆë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì£¼ìš” ì œí’ˆêµ°ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\",\n",
    "        \"details\": [\n",
    "            {\n",
    "                \"series\": \"NEW HX ì‹œë¦¬ì¦ˆ\",\n",
    "                \"features\": \"ìµœì²¨ë‹¨ ê¸°ìˆ ê³¼ ëŸ­ì…”ë¦¬í•œ ë””ìì¸ì´ ê²°í•©ëœ í•˜ì´ì—”ë“œ íŠ¸ë™í„°ë¡œ, ì§ì§„ ììœ¨ì£¼í–‰ ê¸°ëŠ¥ê³¼ ìŠ¤ë§ˆíŠ¸ ì›ê²© ê´€ë¦¬ ì‹œìŠ¤í…œì¸ 'ëŒ€ë™ ì»¤ë„¥íŠ¸'ë¥¼ í†µí•´ ì›ê²© ì œì–´ ë° ê³ ì¥ ì§„ë‹¨ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\",\n",
    "                \"power_range\": \"132~142ë§ˆë ¥\"\n",
    "            },\n",
    "            {\n",
    "                \"series\": \"HX ì‹œë¦¬ì¦ˆ PRIME\",\n",
    "                \"features\": \"ëŒ€í˜• íŠ¸ë™í„°ì˜ ìƒˆë¡œìš´ ê¸°ì¤€ì„ ì œì‹œí•˜ëŠ” í”„ë¼ì„ ëª¨ë¸ë¡œ, ê³ ì¶œë ¥ê³¼ íš¨ìœ¨ì„±ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "                \"usage\": \"ëŒ€í˜• ë°­ì‘ë¬¼, ì¶•ì‚°, ëŒ€í˜• ìˆ˜ë„ì‘ì— ì í•©\"\n",
    "            },\n",
    "            {\n",
    "                \"series\": \"GX ì‹œë¦¬ì¦ˆ\",\n",
    "                \"features\": \"í”„ë¦¬ë¯¸ì—„ ì¤‘í˜• íŠ¸ë™í„°ë¡œ, í•˜ì´í…Œí¬ ê¸°ë°˜ì˜ ë‹¤ì–‘í•œ í¸ì˜ ê¸°ëŠ¥ê³¼ ì¸ì²´ê³µí•™ì  ì„¤ê³„ë¥¼ í†µí•´ ì‘ì—… íš¨ìœ¨ì„±ê³¼ í¸ì˜ì„±ì„ ê·¹ëŒ€í™”í–ˆìŠµë‹ˆë‹¤.\",\n",
    "                \"power_range\": \"60~70ë§ˆë ¥\"\n",
    "            },\n",
    "            {\n",
    "                \"series\": \"RX ì‹œë¦¬ì¦ˆ\",\n",
    "                \"features\": \"ì¡°ì‘, ì—°ë¹„, ì‘ì—…, ê´€ë¦¬ì˜ íš¨ìœ¨ì„±ì„ ì¶”êµ¬í•˜ëŠ” íŠ¸ë™í„°ë¡œ, íŒŒì›Œì…”í‹€ê³¼ ëª¨ë‹ˆí„°5 ë“±ì˜ ê¸°ëŠ¥ì„ íƒ‘ì¬í•˜ì—¬ ê²½ì œì ì¸ ì‘ì—…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\",\n",
    "                \"power_range\": \"59~74ë§ˆë ¥\"\n",
    "            },\n",
    "            {\n",
    "                \"series\": \"NX ì‹œë¦¬ì¦ˆ\",\n",
    "                \"features\": \"ìˆ˜ë„ì‘ê³¼ ë°­ì‘ë¬¼ì— ëª¨ë‘ í™œìš© ê°€ëŠ¥í•œ ì‹¤ìš©ì ì¸ ë³µí•© ë†ì‚¬ìš© íŠ¸ë™í„°ë¡œ, ì „ìì œì–´ ê¸°ëŠ¥ê³¼ ìë™í™” ê¸°ëŠ¥ì„ í†µí•´ ìµœê³ ì˜ ì‘ì—… ëŠ¥ë¥ ê³¼ í¸ì˜ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.\"\n",
    "            },\n",
    "            {\n",
    "                \"series\": \"DK ì‹œë¦¬ì¦ˆ\",\n",
    "                \"features\": \"í•˜ìš°ìŠ¤ ì‘ì—…ì— ìµœì í™”ëœ ì»´íŒ©íŠ¸í•œ íŠ¸ë™í„°ë¡œ, ìµœì†Œ íšŒì „ë°˜ê²½ê³¼ ë†’ì€ ìŠ¹ê°•ë ¥, ì €ìƒ íœë” ë“±ì„ í†µí•´ ì¢ì€ ê³µê°„ì—ì„œë„ íš¨ìœ¨ì ì¸ ì‘ì—…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\"\n",
    "            },\n",
    "            {\n",
    "                \"series\": \"LK ì‹œë¦¬ì¦ˆ\",\n",
    "                \"features\": \"ê³¼ìˆ˜ì› ì‘ì—…ì— ìµœì í™”ëœ íŠ¸ë™í„°ë¡œ, í˜ì´ ì„¸ê³  ì»´íŒ©íŠ¸í•œ ë””ìì¸ì„ í†µí•´ ê³¼ìˆ˜ ì‘ì—…ì˜ íš¨ìœ¨ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.\"\n",
    "            }\n",
    "        ],\n",
    "        \"note\": \"ê° ì‹œë¦¬ì¦ˆëŠ” íŠ¹ì • ì‘ì—… í™˜ê²½ê³¼ ìš”êµ¬ì— ë§ê²Œ ì„¤ê³„ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ì‘ì—… íŠ¹ì„±ì— ë”°ë¼ ì í•©í•œ ëª¨ë¸ì„ ì„ íƒí•˜ì‹œëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Streamlit ë©”ì¸ ì½”ë“œ\n",
    "def main():\n",
    "    # í˜ì´ì§€ ì„¤ì •\n",
    "    st.set_page_config(page_title=\"ëŒ€ë™ AICC ë„ì›€ì´\", layout=\"wide\", page_icon=\"ğŸ¤–\")\n",
    "\n",
    "    st.image('Cute_Robot_Tractor_with_Label.png', width=500)\n",
    "    st.markdown('---')\n",
    "    st.title(\"ì•ˆë…•í•˜ì„¸ìš”! 'ëŒ€ë™ AICC ë„ì›€ì´' ì…ë‹ˆë‹¤\")  # ì‹œì‘ íƒ€ì´í‹€\n",
    "\n",
    "    # ì„¸ì…˜ ì´ˆê¸°í™”\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state[\"messages\"] = []\n",
    "\n",
    "    if \"session_history\" not in st.session_state:\n",
    "        st.session_state[\"session_history\"] = {}\n",
    "\n",
    "    tools = [retriever_tool]\n",
    "\n",
    "    # LLM ì„¤ì •\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # Prompt ì •ì˜\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"\n",
    "                You are a friendly and professional expert with over 10 years of experience at AICC. Your role is to respond to internal staff inquiries in a professional yet approachable manner. All responses should be written in Korean.\n",
    "\n",
    "                You specialize in resolving customer issues, providing product and service information, offering technical support, and guiding staff through various processes. Your current role involves assisting internal staff in handling customer inquiries effectively, leveraging given data to promptly provide solutions.\n",
    "\n",
    "                Analyze the given DataFrame (df) containing the columns Inquiry Type, Customer Name, Inquiry Content, and Received Date. Categorize each inquiry into one of the following categories: [Product Information], [Service Issue], [Technical Support], or [Other]. Additionally, summarize the main points of the customer inquiry and suggest an appropriate solution for internal staff.\n",
    "                Please always include emojis in your responses with a friendly tone.\n",
    "                When the chat begins, first introduce yourself and your goal, and then request information about the type of data you will be processing or any additional details you might need. If the keywords provided by the user do not match the predefined categories, clarify the user's intent, verify if the keywords are suitable, and request clarification if necessary.\n",
    "\n",
    "                Responses should follow the format below:\n",
    "                    \n",
    "                \"Your name is `AICC ë„ì›€ì´`. Please introduce yourself at the beginning of the conversation.\" \n",
    "\n",
    "                #FORMAT\n",
    "\n",
    "                    * ë¬¸ì˜ ìœ í˜•\n",
    "                    -\n",
    "\n",
    "                    * ë¬¸ì˜ ë‹µë³€\n",
    "                    -\n",
    "                    -\n",
    "                    -              \n",
    "                    \n",
    "                \"\"\"\n",
    "\n",
    "            ),\n",
    "            (\"placeholder\", \"{chat_history}\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "            (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ì—ì´ì „íŠ¸ ìƒì„±\n",
    "    agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "    # AgentExecutor ì •ì˜\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬\n",
    "    user_input = st.chat_input('ì§ˆë¬¸ì´ ë¬´ì—‡ì¸ê°€ìš”?')\n",
    "    response = \"\"\n",
    "    if user_input:\n",
    "        session_id = \"default_session\"\n",
    "        session_history = get_session_history(session_id)\n",
    "\n",
    "        # ì¡°ê±´ë¬¸ ìˆ˜ì •\n",
    "        if \"TYM T70\" in user_input:\n",
    "            # ê²½ìŸì‚¬ ì •ë³´ ë°˜í™˜\n",
    "            response = (\n",
    "            f\"**ë¬¸ì˜**: {QUERY_COMPARISON['question']}\\n\\n\"\n",
    "            f\"**ë¬¸ì˜ ìœ í˜•**: {QUERY_COMPARISON['type']}\\n\\n\"\n",
    "            \"\\n**[ë‹µë³€]**\\n\"\n",
    "            f\"1. ì—”ì§„ ì¶œë ¥ ë° ì„±ëŠ¥\\n\"\n",
    "            f\"HX1400L-2C:\\n\"\n",
    "            f\"- ì¶œë ¥: {QUERY_COMPARISON['response']['engine_output_and_performance']['HX1400L-2C']['power']}\\n\"\n",
    "            f\"- ì—”ì§„: {QUERY_COMPARISON['response']['engine_output_and_performance']['HX1400L-2C']['engine']}\\n\"\n",
    "            f\"- ì„¤ëª…: {QUERY_COMPARISON['response']['engine_output_and_performance']['HX1400L-2C']['description']}\\n\"\n",
    "            f\"TYM T70:\\n\"\n",
    "            f\"- ì¶œë ¥: {QUERY_COMPARISON['response']['engine_output_and_performance']['TYM T70']['power']}\\n\"\n",
    "            f\"- ì—”ì§„: {QUERY_COMPARISON['response']['engine_output_and_performance']['TYM T70']['engine']}\\n\"\n",
    "            f\"- ì„¤ëª…: {QUERY_COMPARISON['response']['engine_output_and_performance']['TYM T70']['description']}\\n\"\n",
    "            f\"â†’ ë¹„êµ: {QUERY_COMPARISON['response']['engine_output_and_performance']['comparison']}\\n\"\n",
    "            \"\\n2. ì£¼ìš” ê¸°ëŠ¥ ë° íŠ¹ì§•\\n\"\n",
    "            f\"HX1400L-2C:\\n\"\n",
    "            + \"\\n\".join(f\"- {feature}\" for feature in QUERY_COMPARISON['response']['key_features_and_characteristics']['HX1400L-2C']) + \"\\n\"\n",
    "            f\"TYM T70:\\n\"\n",
    "            + \"\\n\".join(f\"- {feature}\" for feature in QUERY_COMPARISON['response']['key_features_and_characteristics']['TYM T70']) + \"\\n\"\n",
    "            f\"â†’ ë¹„êµ: {QUERY_COMPARISON['response']['key_features_and_characteristics']['comparison']}\\n\"\n",
    "            \"\\n3. ìš©ë„ ë° ì í•©ì„±\\n\"\n",
    "            f\"HX1400L-2C:\\n\"\n",
    "            f\"- ì„¤ëª…: {QUERY_COMPARISON['response']['usage_and_suitability']['HX1400L-2C']['description']}\\n\"\n",
    "            f\"- ìš©ë„: {', '.join(QUERY_COMPARISON['response']['usage_and_suitability']['HX1400L-2C']['applications'])}\\n\"\n",
    "            f\"- ì¥ì : {QUERY_COMPARISON['response']['usage_and_suitability']['HX1400L-2C']['benefits']}\\n\"\n",
    "            f\"TYM T70:\\n\"\n",
    "            f\"- ì„¤ëª…: {QUERY_COMPARISON['response']['usage_and_suitability']['TYM T70']['description']}\\n\"\n",
    "            f\"- í•œê³„: {QUERY_COMPARISON['response']['usage_and_suitability']['TYM T70']['limitations']}\\n\"\n",
    "            f\"â†’ ë¹„êµ: {QUERY_COMPARISON['response']['usage_and_suitability']['comparison']}\\n\"\n",
    "            )\n",
    "        elif \"LK400L5\" in user_input:\n",
    "            # ë°ì´í„° ì¤€ë¹„\n",
    "            details = COMPARE_INFO['answer']['details']\n",
    "\n",
    "            # ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜\n",
    "            details_text = \"\\n\".join([f\"- {item['í•­ëª©']}: {item['ì„¸ë¶€ ë‚´ìš©']}\" for item in details])\n",
    "\n",
    "            # `response`ì— í™”ë©´ ì¶œë ¥ ë‚´ìš©ì„ ëª¨ë‘ ì €ì¥\n",
    "            response = (\n",
    "                f\"**ë¬¸ì˜:** {COMPARE_INFO['question']}\\n\\n\"\n",
    "                f\"**ë¬¸ì˜ ìœ í˜•:** {COMPARE_INFO['type']}\\n\\n\"\n",
    "                f\"**[ë‹µë³€]**\\n\\n\"\n",
    "                f\"{COMPARE_INFO['answer']['description']}\\n\\n\"\n",
    "                f\"ì„¸ë¶€ ì‚¬í•­:\\n{details_text}\"\n",
    "            )\n",
    "        elif \"HX1400L-2C\" in user_input:\n",
    "            # ë°ì´í„° ì¤€ë¹„\n",
    "            details = QUERY_INFO['answer']['details']\n",
    "\n",
    "            # ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜\n",
    "            details_text = \"\\n\".join([f\"- {item['í•­ëª©']}: {item['ì„¸ë¶€ ë‚´ìš©']}\" for item in details])\n",
    "\n",
    "            # `response`ì— í™”ë©´ ì¶œë ¥ ë‚´ìš©ì„ ëª¨ë‘ ì €ì¥\n",
    "            response = (\n",
    "                f\"**ë¬¸ì˜:** {QUERY_INFO['question']}\\n\\n\"\n",
    "                f\"**ë¬¸ì˜ ìœ í˜•:** {QUERY_INFO['type']}\\n\\n\"\n",
    "                f\"**[ë‹µë³€]**\\n\\n\"\n",
    "                f\"{QUERY_INFO['answer']['description']}\\n\\n\"\n",
    "                f\"ì„¸ë¶€ ì‚¬í•­:\\n{details_text}\"\n",
    "            )\n",
    "        elif \"ëŒ€ë™ íŠ¸ë™í„° ì œí’ˆêµ°\" in user_input:\n",
    "            # TRACTOR_INFO ë°ì´í„° ì¤€ë¹„\n",
    "            tractor_details = TRACTOR_INFO[\"answer\"][\"details\"]\n",
    "            description = TRACTOR_INFO[\"answer\"][\"description\"]\n",
    "            note = TRACTOR_INFO[\"answer\"][\"note\"]\n",
    "            \n",
    "            # ê° ì‹œë¦¬ì¦ˆ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "            details_text = \"\\n\\n\".join(\n",
    "                [\n",
    "                    f\"{idx + 1}. {detail['series']}\\n\"\n",
    "                    f\"   - íŠ¹ì§•: {detail['features']}\"\n",
    "                    + (f\"\\n   - ë§ˆë ¥ ë²”ìœ„: {detail['power_range']}\" if \"power_range\" in detail else \"\")\n",
    "                    + (f\"\\n   - ìš©ë„: {detail['usage']}\" if \"usage\" in detail else \"\")\n",
    "                    for idx, detail in enumerate(tractor_details)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # `response`ì— í™”ë©´ ì¶œë ¥ ë‚´ìš©ì„ ëª¨ë‘ ì €ì¥\n",
    "            response = (\n",
    "                f\"**ë¬¸ì˜:** {TRACTOR_INFO['question']}\\n\\n\"\n",
    "                f\"**ë¬¸ì˜ ìœ í˜•:** {TRACTOR_INFO['type']}\\n\\n\"\n",
    "                f\"**[ë‹µë³€]**\\n\\n\"\n",
    "                f\"{description}\\n\\n\"\n",
    "                f\"{details_text}\\n\\n\"\n",
    "                f\"{note}\"\n",
    "            )\n",
    "        else:\n",
    "            # ì—ì´ì „íŠ¸ ì‹¤í–‰\n",
    "            if session_history.messages:\n",
    "                previous_messages = [{\"role\": msg['role'], \"content\": msg['content']} for msg in session_history.messages]\n",
    "                response = chat_with_agent(user_input + \"\\n\\nPrevious Messages: \" + str(previous_messages), agent_executor)\n",
    "            else:\n",
    "                response = chat_with_agent(user_input, agent_executor)\n",
    "\n",
    "        # ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ì— ì¶”ê°€\n",
    "        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": user_input})\n",
    "        st.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "        # ì„¸ì…˜ ê¸°ë¡ì— ë©”ì‹œì§€ë¥¼ ì¶”ê°€\n",
    "        session_history.add_message({\"role\": \"user\", \"content\": user_input})\n",
    "        session_history.add_message({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "\n",
    "    # ëŒ€í™” ë‚´ìš© ì¶œë ¥\n",
    "    print_messages()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in c:\\users\\thrje\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ympy (c:\\Users\\thrje\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (c:\\Users\\thrje\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (c:\\Users\\thrje\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
